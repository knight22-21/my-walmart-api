{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a516c8b-f156-4616-98e8-bb7c929dab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:11:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Krish\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 640, number of negative: 7360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1052\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080000 -> initscore=-2.442347\n",
      "[LightGBM] [Info] Start training from score -2.442347\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "CatBoost Accuracy: 1.0000\n",
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1840\n",
      "           1       1.00      1.00      1.00       160\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "\n",
      "XGBoost Accuracy: 0.9995\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1840\n",
      "           1       1.00      0.99      1.00       160\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "\n",
      "LightGBM Accuracy: 0.9995\n",
      "LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1840\n",
      "           1       0.99      1.00      1.00       160\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 7360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1052\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080000 -> initscore=-2.442347\n",
      "[LightGBM] [Info] Start training from score -2.442347\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "ðŸ§  Voting Ensemble Accuracy: 1.0000\n",
      "Voting Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1840\n",
      "           1       1.00      1.00      1.00       160\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:11:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\Krish\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Load & Prepare Training Data ===\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Select only the required base columns\n",
    "base_cols = [\n",
    "    \"user_id\", \"total_orders\", \"total_returns\", \"days_to_return_avg\",\n",
    "    \"high_value_returns\", \"category_return_ratio\", \"exchange_ratio\", \"damaged_returns\"\n",
    "]\n",
    "\n",
    "df = df[base_cols + ['label']].copy()\n",
    "df['user_id'] = df['user_id'].astype(str).str[-5:]\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# === 2. Feature Engineering ===\n",
    "df['return_rate'] = df['total_returns'] / (df['total_orders'] + 1e-5)\n",
    "df['fast_return_flag'] = (df['days_to_return_avg'] < 3).astype(int)\n",
    "\n",
    "# Drop leaky features\n",
    "df.drop(columns=['user_id', 'high_value_returns', 'damaged_returns'], inplace=True, errors='ignore')\n",
    "\n",
    "# Encode categoricals (if any)\n",
    "cat_cols = ['location', 'device_fingerprint']\n",
    "cat_cols = [col for col in cat_cols if col in df.columns]  # Handle absence\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "if cat_cols:\n",
    "    df[cat_cols] = ordinal_encoder.fit_transform(df[cat_cols])\n",
    "    joblib.dump(ordinal_encoder, 'ordinal_encoder.pkl')\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "joblib.dump(X.columns.tolist(), 'feature_columns.pkl')\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === 3. Initialize Models ===\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "lgb = LGBMClassifier()\n",
    "\n",
    "# Fit individual models\n",
    "cat.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "# Save individual models (optional)\n",
    "joblib.dump(cat, 'catboost_model.pkl')\n",
    "joblib.dump(xgb, 'xgboost_model.pkl')\n",
    "joblib.dump(lgb, 'lightgbm_model.pkl')\n",
    "\n",
    "# Evaluate individual models\n",
    "for name, model in zip(['CatBoost', 'XGBoost', 'LightGBM'], [cat, xgb, lgb]):\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n{name} Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "    print(f\"{name} Classification Report:\\n{classification_report(y_test, preds)}\")\n",
    "\n",
    "# === 4. Voting Ensemble ===\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('cat', cat), ('xgb', xgb), ('lgb', lgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_train, y_train)\n",
    "joblib.dump(ensemble, 'voting_ensemble_model.pkl')\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_preds = ensemble.predict(X_test)\n",
    "print(f\"\\nðŸ§  Voting Ensemble Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}\")\n",
    "print(f\"Voting Ensemble Classification Report:\\n{classification_report(y_test, ensemble_preds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d32564-880b-4814-bc57-7174941db8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krish\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Ensemble Accuracy on test.csv: 0.9700\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      2530\n",
      "           1       1.00      0.81      0.89       470\n",
      "\n",
      "    accuracy                           0.97      3000\n",
      "   macro avg       0.98      0.90      0.94      3000\n",
      "weighted avg       0.97      0.97      0.97      3000\n",
      "\n",
      "\n",
      "ðŸ” Sample Predictions:\n",
      "  user_id  prediction  True_Label\n",
      "0   00028           0           0\n",
      "1   00830           0           0\n",
      "2   00501           0           0\n",
      "3   01967           0           0\n",
      "4   01636           0           0\n",
      "5   02444           0           0\n",
      "6   00148           1           1\n",
      "7   01197           0           0\n",
      "8   00012           0           0\n",
      "9   01622           0           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# === 1. Load & Prepare Test Data ===\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Select same base columns\n",
    "base_cols = [\n",
    "    \"user_id\", \"total_orders\", \"total_returns\", \"days_to_return_avg\",\n",
    "    \"high_value_returns\", \"category_return_ratio\", \"exchange_ratio\", \"damaged_returns\"\n",
    "]\n",
    "test_df = test_df[base_cols + (['label'] if 'label' in test_df.columns else [])].copy()\n",
    "test_df['user_id'] = test_df['user_id'].astype(str).str[-5:]\n",
    "\n",
    "# Feature engineering\n",
    "test_df['return_rate'] = test_df['total_returns'] / (test_df['total_orders'] + 1e-5)\n",
    "test_df['fast_return_flag'] = (test_df['days_to_return_avg'] < 3).astype(int)\n",
    "\n",
    "# Save user IDs\n",
    "user_ids = test_df['user_id']\n",
    "\n",
    "# Drop leaky features\n",
    "test_df.drop(columns=['user_id', 'high_value_returns', 'damaged_returns'], inplace=True, errors='ignore')\n",
    "\n",
    "# Load and apply encoder\n",
    "cat_cols = ['location', 'device_fingerprint']\n",
    "cat_cols = [col for col in cat_cols if col in test_df.columns]\n",
    "if cat_cols:\n",
    "    ordinal_encoder = joblib.load('ordinal_encoder.pkl')\n",
    "    test_df[cat_cols] = ordinal_encoder.transform(test_df[cat_cols])\n",
    "\n",
    "# Extract true labels if available\n",
    "has_label = 'label' in test_df.columns\n",
    "if has_label:\n",
    "    y_test_true = test_df['label'].astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_test_raw = test_df.drop(columns=['label'], errors='ignore')\n",
    "\n",
    "# === Fix: Align columns with training ===\n",
    "expected_cols = joblib.load(\"feature_columns.pkl\")\n",
    "\n",
    "# Add missing columns\n",
    "for col in expected_cols:\n",
    "    if col not in X_test_raw.columns:\n",
    "        X_test_raw[col] = 0\n",
    "\n",
    "# Reorder columns\n",
    "X_test_raw = X_test_raw[expected_cols]\n",
    "\n",
    "# Load scaler and scale\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "# Load ensemble model\n",
    "ensemble = joblib.load('voting_ensemble_model.pkl')\n",
    "\n",
    "# Predict\n",
    "preds = ensemble.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate if ground truth exists\n",
    "if has_label:\n",
    "    acc = accuracy_score(y_test_true, preds)\n",
    "    print(f\"\\nâœ… Ensemble Accuracy on test.csv: {acc:.4f}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test_true, preds)}\")\n",
    "\n",
    "# Save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'prediction': preds\n",
    "})\n",
    "\n",
    "if has_label:\n",
    "    results_df['True_Label'] = y_test_true\n",
    "\n",
    "print(\"\\nðŸ” Sample Predictions:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Optional: Save\n",
    "# results_df.to_csv(\"final_test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58de77-4b02-47dc-a3d6-3c67f3442dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
